\documentclass{article}


\usepackage{listings}
\usepackage{xcolor}
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
% \usepackage[a4paper,top=1.5cm,bottom=1.7cm,left=1cm,right=1cm,marginparwidth=1.75cm]{geometry}
\usepackage[a4paper, margin=0.72in]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{COL216 Semester 2024-25 Sem II Assignment - 3}

\author{
    Aadi Govil (2023CS10490) |
    Apurva Samota (2023CS10585)
}


\begin{document}

\maketitle

\section{Assumptions:}
\begin{enumerate}
    \item LRU cache - cache line eviction is based on LRU i.e. least-recently used 
    \item Transaction arbitration is being done according to the lowest index.
    \item To halt a core on writeback/transfer for another core, we have added a toggler so we can run both by changing one global variable - \texttt{IF\_EVICT\_OR\_TRANSFER\_STALL}.
    \item There can be only one active bus transaction at a time - either writeback/transfer or invalidate.
    \item Checking neighbors in bus transactions is considered almost instantaneous, i.e. one can check neighbors \& get started on data transfer in the same cycle.
    \item Per cycle, only one processing can be done on bus.
    \item If there is miss at cycle T : then it will get back data in cycle T+100 \& performs hit on data in the same cycle.
\end{enumerate}

\section{Definitions:}
\begin{enumerate}
    \item Execution Cycles: Cycles in which either it computes an instruction or the bus is doing something to help compute its instruction. These may include write-backs. This is only till PC is less than no. of instructions for the respective core.
    \item Idle Cycles: All Cycles that occur before PC exceed the size of the instructions vector but which are not counted in the execution cycles.
    \item Misses: This can be at most one per instruction.
    \item Cache evictions: When M/E/S block becomes I.
    \item Writeback: When M block becomes I.
    \item Invalidations: Any \texttt{RdX} or Invalidate signal that a core sends out to the bus.
    \item Data Traffic: Any input/output cacheline going in/out of the core.
    \item Bus Traffic: This includes any cachelines transferred to the bus for any transaction: memory to cache, cache to cache, or writebacks.
    
\end{enumerate}


\section{Implementation Details:}
\subsection{Main Classes:}
\begin{enumerate}
    \item \texttt{cacheLine}: Represents a single cache block. Stores tag, state,(eg. Modified, Shared, Exclusive, Invalid), and line size bits.
    \item \texttt{cacheSet}: Contains multiple \texttt{cacheLine} entries. Supports tag lookup and LRU-based replacement. Maintains an LRU list using a doubly-linked list.
    \item \texttt{cache}: Represents a single cache core. Stores \texttt{cacheSet} entries and processes memory read/write instructions. Handles cache hit/miss detection, MESI  state transitions, and bus requests.
    \item \texttt{bus}: Simulates a shared bus across quad cores. Handles arbutration, memory fetches, invalidations, and updates during \texttt{Rd}/\texttt{RdX}/\texttt{WriteInvalidate} transactions.
\end{enumerate}






\subsection{Data Structures Used:}
\begin{enumerate}
    \item \texttt{std::vector}: Used to store: instructions, cache lines within set, sets within each cache.
\end{enumerate}




\section{Flow-Chart for functions:}
\subsection{\texttt{bus::runForACycle}} : 
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{runForACycle.png}
  \caption{\texttt{bus::runForACycle} work-flow}
  \label{fig:arch}
\end{figure}
\subsection{\texttt{cache::processSnoop}} :
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{processSnoop.png}
  \caption{\texttt{cache::processSnoop} work-flow}
  \label{fig:arch}
\end{figure}
\subsection{\texttt{cache::processInst}} :
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{processInst.png}
  \caption{\texttt{cache::processInst} work-flow}
  \label{fig:arch}
\end{figure}
\section{Experiments:}
\subsection{Varying associativity:}
We fixed setBits at 6 and lineSizeBits at 5 and varied associativity E from 1,2,4,8,16 which lead to the following results from app1:
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{plot_vary_E.png}
  \caption{Varying associativity}
  \label{fig:arch}
\end{figure}
This result is obtained because you can store more cache lines that map to the same set as the associativity decreases, which helps improve time by reducing the number of cache write backs to the memory and improving the number of cache hits.

\subsection{Varying number of sets per cache:}
We fixed associativity at 2 and lineSizeBits at 5 and varied setBits from 1 to 6 which lead to the following results from app1:
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{plot_vary_s.png}
  \caption{Varying set bits}
  \label{fig:arch}
\end{figure}
As the associativity for the cache is very small so for small values of set bits, the amount of space available to the cache is very small. Moreover, more is the amount of set bits, more is the amount of sets, and hence lower is the false sharing, which helps reduce the amount of invalidates between 2 caches as well as evictions within the same cache.

\subsection{Varying number of bytes per cache line:}
We fixed associativity at 2 and setSizeBits at 6 and varied setBits from 2 to 6 which lead to the following results from app1:
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{plot_vary_b.png}
  \caption{Varying line size bits}
  \label{fig:arch}
\end{figure}
As the amount of bits per cache line increase, the locality of running program gets better, i.e, if I got hit on me and then my neighbouring word is called down the line, I would not hit in case the number of set bits is lesser, but would hit if the number of set bits is higher. This causes an decrease in the amount of cycles taken for me to run the program app1.


\section{Interesting hand generated trace}

We have added a trace test to the submission. What is interesting about this trace? \\

This trace is a small trace which gives different outputs for both the versions of our assignment submissions, even though running for the same amount of cycles.\\

Secondly it features false sharing heavily, and we can see its effects.\\

Thirdly we can see the graph given by it on varying the number of bytes per cache line which is very interesting to see as well.\\

\subsection{Interesting behaviour comparing the two versions:}
In the version where processes like cache to cache transfer and cache writebacks go on without halting the sender, this trace gave an interesting output where in everything relating to cache 1 finished before cache 0 finished its instructions.\\
Lets analyse this for both versions: \\
In the 0th cycle, all caches miss and send the an RdX request to the bus. This leads to cache 0 winning this request, and all the other caches halting, till cycle 100.\\
In cycle 100, as the cache 0 gets back the cache line it wanted, it issues a write hit and sets its copy to M. In this cycle it does not send any request to the bus, and the bus being free, gets taken hold of by cache number 1, which sees that cache 0 has an M copy of the cache line that it wants. \\
Here's where it diverges:
In the case where the cache does not stall when serving someone else's requests: \\
Hence, now cache 0 sends back this block to the memory, invalidating it inside of itself and causing a read miss on the next cycle of its own execution, cuing up its own request to the bus. This is a prime example of false sharing. Now cache number 1 has control of the bus for the next 200 cycles, firstly to get cache 0 to deposit its memory to main memory and then get the block back from main memory for cache 1.
When this instruction for the cache 1 finishes up, cache 0 already has a bus transaction waiting while the cache 1 goes ahead and performs a hit on the retrieved block. The bus transaction starts requesting block 1 for its modified copy to read, and hence block 1 has to set its copy of the block to S. Thus when in the next cycle there is a read hit on cache 1, thus ending all its instructions before the instructions of cache 0 complete.\\
Thus the cache 0 goes and completes it's second instruction, performs a read hit.\\
\\
In the case where the cache does stall when serving someone else's requests:\\
Now cache 0 cannot run its second instruction as it has to send its memory block to the main memory and we are supposed to halt for it in this implementation. So the cache 1 completes its bus transaction in 200 cycles.\\
Now once it's done, cache 0 gets to run its own read instruction and sends a read miss request to the bus which then accepts and starts processing it. This has a hit on cache 1 where the line has been modified, so a cache 1 to memory transfer occurs, after which a cache 1 to cache 0 transfer occurs. This whole while cache 1 is blocked not able to access its own last instruction.\\
Once the bus transaction is done, on cycle number 416, both the caches are finally able to process their reads, which are both hits, and thus they end their execution on the same cycle, providing the other caches with the cache block later on, but ending their own instructions here.\\
\\
\\

\subsection{Interesting behaviour on varying b}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{plot_vary_b1.png}
  \caption{Varying line size bits on the interesting trace}
  \label{fig:arch}
\end{figure}

Contrary to what we had seen earlier in the report, here increasing the amount of line size bits increased the amount of cycles for the program to run.\\
The reason: False sharing.\\
False sharing being on the higher side actually incentivizes us to use lower amount of line size bits as that removes the conflicts that exist between any 2 caches. Thus in the case where the cacheline is just one word long, the trace runs much faster, almost in half the time, as the reads in all the files can occur immediately after the writes as there is no false sharing between them.\\



\end{document}
